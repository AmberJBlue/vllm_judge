{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import Judge, JudgeConfig, Metric, BUILTIN_METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['helpfulness', 'accuracy', 'clarity', 'conciseness', 'relevance', 'safety', 'toxicity', 'code_quality', 'code_security', 'creativity', 'professionalism', 'educational_value', 'preference', 'appropriate', 'factual', 'medical_accuracy', 'legal_appropriateness', 'educational_content_template', 'code_review_template', 'customer_service_template', 'writing_quality_template', 'product_review_template', 'medical_info_template', 'api_docs_template'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUILTIN_METRICS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = Judge.from_url(base_url=\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await judge.evaluate(content=\"I want to bump the version to 1.0.1, is it a good idea?\",\n",
    "                           criteria=\"Check the professional tone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': False,\n",
       " 'reasoning': 'The response lacks a professional tone and is informal. It uses casual language and lacks context or formal structure.',\n",
       " 'score': None,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": false,\\n    \"reasoning\": \"The response lacks a professional tone and is informal. It uses casual language and lacks context or formal structure.\",\\n    \"score\": null\\n}'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 5,\n",
       " 'reasoning': 'The response lacks a formal and professional tone. It uses informal language and an interrogative form which is not typical in professional communication.',\n",
       " 'score': 5.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": 5,\\n    \"reasoning\": \"The response lacks a formal and professional tone. It uses informal language and an interrogative form which is not typical in professional communication.\",\\n    \"score\": 5\\n}'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = await judge.evaluate(content=\"I want to bump the version to 1.0.1, is it a good idea?\",\n",
    "                           criteria=\"Check the professional tone.\",\n",
    "                           rubric=\"Assign a score between 0 and 10 based on the professional tone. 0 is the worst and 10 is the best.\")\n",
    "res.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 5,\n",
       " 'reasoning': 'The response is somewhat direct and to the point, but lacks formality and context typically expected in a professional setting.',\n",
       " 'score': 5.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": 5,\\n    \"reasoning\": \"The response is somewhat direct and to the point, but lacks formality and context typically expected in a professional setting.\",\\n    \"score\": 5\\n}'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = await judge.evaluate(content=\"I want to bump the version to 1.0.1, is it a good idea?\",\n",
    "                           criteria=\"Check the professional tone.\",\n",
    "                           rubric={\n",
    "                               0: \"The response is not professional.\",\n",
    "                               5: \"The response is somewhat professional.\",\n",
    "                               10: \"The response is very professional.\"\n",
    "                           })\n",
    "res.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "professional_tone_metric = Metric(\n",
    "    name=\"professional_tone\",\n",
    "    criteria=\"Assess the professional tone of the provided email body.\",\n",
    "    rubric=\"Classify the text into 'professional', 'moderate', or 'non-professional' categories.\",\n",
    "    scale=(1,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'moderate',\n",
       " 'reasoning': 'The email lacks formal language and a clear request, which is more appropriate for a professional setting. It is direct but informal.',\n",
       " 'score': 5.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"moderate\",\\n    \"reasoning\": \"The email lacks formal language and a clear request, which is more appropriate for a professional setting. It is direct but informal.\",\\n    \"score\": 5\\n}'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = await judge.evaluate(response=\"I want to bump the version to 1.0.1, is it a good idea?\",\n",
    "                           metric=professional_tone_metric)\n",
    "res.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'non-professional',\n",
       " 'reasoning': 'The response uses informal and expletive language, which is not appropriate for a professional context.',\n",
       " 'score': 1.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"non-professional\",\\n    \"reasoning\": \"The response uses informal and expletive language, which is not appropriate for a professional context.\",\\n    \"score\": 1\\n}'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = await judge.evaluate(response=\"Holy shit, this is a great!\",\n",
    "                           metric=professional_tone_metric)\n",
    "res.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge.api import JudgeClient\n",
    "\n",
    "client = JudgeClient(\"http://localhost:9090\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'healthy',\n",
       " 'version': '0.1.0',\n",
       " 'model': 'qwen2',\n",
       " 'base_url': 'http://localhost:8080',\n",
       " 'uptime_seconds': 62.64390587806702,\n",
       " 'total_evaluations': 1,\n",
       " 'active_connections': 0,\n",
       " 'metrics_available': 24}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.health_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': False,\n",
       " 'reasoning': 'The response lacks technical detail and does not provide a substantive explanation of why Python is great.',\n",
       " 'score': None,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": false,\\n    \"reasoning\": \"The response lacks technical detail and does not provide a substantive explanation of why Python is great.\",\\n    \"score\": null\\n}'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await client.evaluate(\n",
    "    response=\"Python is great!\",\n",
    "    criteria=\"technical accuracy\"\n",
    ")\n",
    "result.model_dump() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm-judge-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
