{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import Judge, Metric, TemplateProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = Judge.from_url(base_url=\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 5,\n",
       " 'reasoning': 'The function is concise but uses a naive recursive approach which is inefficient and can lead to a stack overflow for large values of n. It lacks error handling and docstrings for documentation.',\n",
       " 'score': 5.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": 5,\\n    \"reasoning\": \"The function is concise but uses a naive recursive approach which is inefficient and can lead to a stack overflow for large values of n. It lacks error handling and docstrings for documentation.\",\\n    \"score\": 5\\n}',\n",
       "  'template_vars': {'language': 'Python', 'use_case': 'production deployment'},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic usage with format strings\n",
    "result = await judge.evaluate(\n",
    "    response=\"def fibonacci(n): return n if n <= 1 else fib(n-1) + fib(n-2)\",\n",
    "    criteria=\"Evaluate this {language} function for {use_case}\",\n",
    "    template_vars={\n",
    "        \"language\": \"Python\",\n",
    "        \"use_case\": \"production deployment\"\n",
    "    },\n",
    "    rubric=\"Assign a score between 1 and 10 based on the quality of the code, with 1 being the worst and 10 being the best\"\n",
    ")\n",
    "result.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define once\n",
    "code_review_metric = Metric(\n",
    "    name=\"code_review\",\n",
    "    criteria=\"Review this {language} code for {purpose}\",\n",
    "    rubric={\n",
    "        10: \"Perfect {language} code for {purpose}\",\n",
    "        5: \"Acceptable for {purpose} with improvements\",\n",
    "        1: \"Unsuitable for {purpose}\"\n",
    "    },\n",
    "    system_prompt=\"You are a {language} expert.\",\n",
    "    required_vars=[\"language\", \"purpose\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_code = \"def fibonacci(n): return n if n <= 1 else fib(n-1) + fib(n-2)\"\n",
    "js_code = \"function fibonacci(n) { return n <= 1 ? n : fibonacci(n-1) + fibonacci(n-2); }\"\n",
    "\n",
    "\n",
    "# Use many times with different contexts\n",
    "result1 = await judge.evaluate(\n",
    "    response=python_code,\n",
    "    metric=code_review_metric,\n",
    "    template_vars={\"language\": \"Python\", \"purpose\": \"data science\"}\n",
    ")\n",
    "\n",
    "result2 = await judge.evaluate(\n",
    "    response=js_code,\n",
    "    metric=code_review_metric,\n",
    "    template_vars={\"language\": \"JavaScript\", \"purpose\": \"web frontend\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 1,\n",
       " 'reasoning': \"The function does not perform any data science tasks and incorrectly uses the name 'fib' instead of 'fibonacci' in the recursive call, which will cause an error.\",\n",
       " 'score': None,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": 1,\\n    \"reasoning\": \"The function does not perform any data science tasks and incorrectly uses the name \\'fib\\' instead of \\'fibonacci\\' in the recursive call, which will cause an error.\",\\n    \"score\": null\\n}',\n",
       "  'template_vars': {'language': 'Python', 'purpose': 'data science'},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 5,\n",
       " 'reasoning': 'The function is a correct implementation of the Fibonacci sequence, but it lacks an optimization for performance and does not consider edge cases like negative numbers or non-integer inputs.',\n",
       " 'score': 5.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": 5,\\n    \"reasoning\": \"The function is a correct implementation of the Fibonacci sequence, but it lacks an optimization for performance and does not consider edge cases like negative numbers or non-integer inputs.\",\\n    \"score\": 5\\n}',\n",
       "  'template_vars': {'language': 'JavaScript', 'purpose': 'web frontend'},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional logic in templates\n",
    "api_review = Metric(\n",
    "    name=\"api_review\",\n",
    "    criteria=\"\"\"\n",
    "    Review this API endpoint:\n",
    "    {% for aspect in aspects %}\n",
    "    - {{ aspect }}\n",
    "    {% endfor %}\n",
    "    {% if security_critical %}\n",
    "    Pay special attention to authentication and authorization.\n",
    "    {% endif %}\n",
    "    \"\"\",\n",
    "    rubric=\"Classify as GOOD, DECENT, BAD and assign a score between 1 and 10 based on the quality of the code, with 1 being the worst and 10 being the best\",\n",
    "    template_engine=\"jinja2\"\n",
    ")\n",
    "\n",
    "api_code = \"\"\"\n",
    "@app.route('/api/v1/users')\n",
    "def get_users():\n",
    "    # Get all users\n",
    "    return jsonify(users)\n",
    "\"\"\"\n",
    "\n",
    "result = await judge.evaluate(\n",
    "    response=api_code,\n",
    "    metric=api_review,\n",
    "    template_vars={\n",
    "        \"aspects\": [\"RESTful design\", \"Error handling\", \"Documentation\"],\n",
    "        \"security_critical\": True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'DECENT',\n",
       " 'reasoning': 'The code is simple and returns a JSON response, but it lacks RESTful design principles, error handling, and proper documentation. There is no indication of authentication or authorization, which are critical for API security.',\n",
       " 'score': 4.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"DECENT\",\\n    \"reasoning\": \"The code is simple and returns a JSON response, but it lacks RESTful design principles, error handling, and proper documentation. There is no indication of authentication or authorization, which are critical for API security.\",\\n    \"score\": 4\\n}',\n",
       "  'template_vars': {'aspects': ['RESTful design',\n",
       "    'Error handling',\n",
       "    'Documentation'],\n",
       "   'security_critical': True},\n",
       "  'template_engine': 'jinja2'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric with defaults\n",
    "education_metric = Metric(\n",
    "    name=\"education\",\n",
    "    criteria=\"Evaluate for {grade_level} studying {subject}\",\n",
    "    template_vars={\n",
    "        \"grade_level\": \"high school\"  # Default\n",
    "    },\n",
    "    rubric=\"Classify as GOOD, DECENT, BAD and assign a score between 1 and 10 based on the quality of the code, with 1 being the worst and 10 being the best\",\n",
    "    required_vars=[\"subject\"]  # Only subject is required\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'BAD',\n",
       " 'reasoning': \"The response does not address the evaluation of high school biology or provide any code for assessment. It discusses a travel agency's need for culturally appropriate translations, which is unrelated to the given context.\",\n",
       " 'score': None,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"BAD\",\\n    \"reasoning\": \"The response does not address the evaluation of high school biology or provide any code for assessment. It discusses a travel agency\\'s need for culturally appropriate translations, which is unrelated to the given context.\",\\n    \"score\": null\\n}',\n",
       "  'template_vars': {'grade_level': 'high school', 'subject': 'biology'},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = \"The travel agency wants to ensure their translations are not only accurate but also culturally appropriate. To achieve this they are considering creating a custom metric that allows Worldwide WanderAgency to quantify how well their translations maintain cultural context and idiomatic expressions.\"\n",
    "\n",
    "# Use with default grade_level\n",
    "result = await judge.evaluate(\n",
    "    response=content,\n",
    "    metric=education_metric,\n",
    "    template_vars={\"subject\": \"biology\"}\n",
    ")\n",
    "\n",
    "result.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'DECENT',\n",
       " 'reasoning': 'The response provides a clear idea of what the travel agency is aiming to achieve, which is to ensure translations are culturally appropriate. However, it does not provide any technical details about how the custom metric would be implemented, which is crucial for evaluating the quality of the code. Therefore, it is considered decent as it sets the right direction but lacks technical depth.',\n",
       " 'score': 6.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"DECENT\",\\n    \"reasoning\": \"The response provides a clear idea of what the travel agency is aiming to achieve, which is to ensure translations are culturally appropriate. However, it does not provide any technical details about how the custom metric would be implemented, which is crucial for evaluating the quality of the code. Therefore, it is considered decent as it sets the right direction but lacks technical depth.\",\\n    \"score\": 6\\n}',\n",
       "  'template_vars': {'grade_level': 'undergraduate',\n",
       "   'subject': 'computer science'},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Override grade_level\n",
    "result = await judge.evaluate(\n",
    "    response=content,\n",
    "    metric=education_metric,\n",
    "    template_vars={\n",
    "        \"subject\": \"computer science\",\n",
    "        \"grade_level\": \"undergraduate\"  # Override default\n",
    "    }\n",
    ")\n",
    "result.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm-judge-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
