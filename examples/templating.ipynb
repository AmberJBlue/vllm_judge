{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import Judge, Metric, TemplateProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = Judge.from_url(base_url=\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'FAIL',\n",
       " 'reasoning': \"The function contains a typo ('fib' instead of 'fibonacci' in the recursive call) and lacks a proper docstring or comments, which are essential for production code. The code is also not optimized and may lead to a stack overflow for large values of n due to repeated calculations.\",\n",
       " 'score': 3.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"FAIL\",\\n    \"reasoning\": \"The function contains a typo (\\'fib\\' instead of \\'fibonacci\\' in the recursive call) and lacks a proper docstring or comments, which are essential for production code. The code is also not optimized and may lead to a stack overflow for large values of n due to repeated calculations.\",\\n    \"score\": 3\\n}',\n",
       "  'template_vars': {'language': 'Python', 'use_case': 'production deployment'},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic usage with format strings\n",
    "result = await judge.evaluate(\n",
    "    content=\"def fibonacci(n): return n if n <= 1 else fib(n-1) + fib(n-2)\",\n",
    "    criteria=\"Evaluate this {language} function for {use_case}\",\n",
    "    template_vars={\n",
    "        \"language\": \"Python\",\n",
    "        \"use_case\": \"production deployment\"\n",
    "    },\n",
    "    rubric=\"Assign a score between 1 and 10 based on the quality of the code, with 1 being the worst and 10 being the best\"\n",
    ")\n",
    "result.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define once\n",
    "code_review_metric = Metric(\n",
    "    name=\"code_review\",\n",
    "    criteria=\"Review this {language} code for {purpose}\",\n",
    "    rubric={\n",
    "        10: \"Perfect {language} code for {purpose}\",\n",
    "        5: \"Acceptable for {purpose} with improvements\",\n",
    "        1: \"Unsuitable for {purpose}\"\n",
    "    },\n",
    "    system_prompt=\"You are a {language} expert.\",\n",
    "    required_vars=[\"language\", \"purpose\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_code = \"def fibonacci(n): return n if n <= 1 else fib(n-1) + fib(n-2)\"\n",
    "js_code = \"function fibonacci(n) { return n <= 1 ? n : fibonacci(n-1) + fibonacci(n-2); }\"\n",
    "\n",
    "\n",
    "# Use many times with different contexts\n",
    "result1 = await judge.evaluate(\n",
    "    content=python_code,\n",
    "    metric=code_review_metric,\n",
    "    template_vars={\"language\": \"Python\", \"purpose\": \"data science\"}\n",
    ")\n",
    "\n",
    "result2 = await judge.evaluate(\n",
    "    content=js_code,\n",
    "    metric=code_review_metric,\n",
    "    template_vars={\"language\": \"JavaScript\", \"purpose\": \"web frontend\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'False',\n",
       " 'reasoning': 'The code is a recursive implementation of the Fibonacci sequence, but it lacks an appropriate base case check and is inefficient for larger values of n due to repeated calculations. This makes it unsuitable for data science applications where performance and efficiency are crucial.',\n",
       " 'score': 1.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"False\",\\n    \"reasoning\": \"The code is a recursive implementation of the Fibonacci sequence, but it lacks an appropriate base case check and is inefficient for larger values of n due to repeated calculations. This makes it unsuitable for data science applications where performance and efficiency are crucial.\",\\n    \"score\": 1\\n}',\n",
       "  'template_vars': {'language': 'Python', 'purpose': 'data science'},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'false',\n",
       " 'reasoning': 'The function is a correct implementation of the Fibonacci sequence, but it is not efficient for large values of n due to its exponential time complexity. This makes it unsuitable for web frontend applications where performance is critical.',\n",
       " 'score': 1.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"false\",\\n    \"reasoning\": \"The function is a correct implementation of the Fibonacci sequence, but it is not efficient for large values of n due to its exponential time complexity. This makes it unsuitable for web frontend applications where performance is critical.\",\\n    \"score\": 1\\n}',\n",
       "  'template_vars': {'language': 'JavaScript', 'purpose': 'web frontend'},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional logic in templates\n",
    "api_review = Metric(\n",
    "    name=\"api_review\",\n",
    "    criteria=\"\"\"\n",
    "    Review this API endpoint:\n",
    "    {% for aspect in aspects %}\n",
    "    - {{ aspect }}\n",
    "    {% endfor %}\n",
    "    {% if security_critical %}\n",
    "    Pay special attention to authentication and authorization.\n",
    "    {% endif %}\n",
    "    \"\"\",\n",
    "    rubric=\"Classify as GOOD, DECENT, BAD and assign a score between 1 and 10 based on the quality of the code, with 1 being the worst and 10 being the best\",\n",
    "    template_engine=\"jinja2\"\n",
    ")\n",
    "\n",
    "api_code = \"\"\"\n",
    "@app.route('/api/v1/users')\n",
    "def get_users():\n",
    "    # Get all users\n",
    "    return jsonify(users)\n",
    "\"\"\"\n",
    "\n",
    "result = await judge.evaluate(\n",
    "    content=api_code,\n",
    "    metric=api_review,\n",
    "    template_vars={\n",
    "        \"aspects\": [\"RESTful design\", \"Error handling\", \"Documentation\"],\n",
    "        \"security_critical\": True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'DECENT',\n",
       " 'reasoning': 'The API endpoint follows a basic RESTful design by using the GET method for retrieving users. However, it lacks error handling and documentation, which are crucial for a robust API. Additionally, there is no mention of authentication and authorization, which are essential for securing the API.',\n",
       " 'score': 5.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"DECENT\",\\n    \"reasoning\": \"The API endpoint follows a basic RESTful design by using the GET method for retrieving users. However, it lacks error handling and documentation, which are crucial for a robust API. Additionally, there is no mention of authentication and authorization, which are essential for securing the API.\",\\n    \"score\": 5\\n}',\n",
       "  'template_vars': {'aspects': ['RESTful design',\n",
       "    'Error handling',\n",
       "    'Documentation'],\n",
       "   'security_critical': True},\n",
       "  'template_engine': 'jinja2'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric with defaults\n",
    "education_metric = Metric(\n",
    "    name=\"education\",\n",
    "    criteria=\"Evaluate for {grade_level} studying {subject}\",\n",
    "    template_vars={\n",
    "        \"grade_level\": \"high school\"  # Default\n",
    "    },\n",
    "    rubric=\"Classify as GOOD, DECENT, BAD and assign a score between 1 and 10 based on the quality of the code, with 1 being the worst and 10 being the best\",\n",
    "    required_vars=[\"subject\"]  # Only subject is required\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'BAD',\n",
       " 'reasoning': \"The response does not address the evaluation of high school biology or provide any code for assessment. It discusses a travel agency's need for culturally appropriate translations, which is unrelated to the given context.\",\n",
       " 'score': None,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"BAD\",\\n    \"reasoning\": \"The response does not address the evaluation of high school biology or provide any code for assessment. It discusses a travel agency\\'s need for culturally appropriate translations, which is unrelated to the given context.\",\\n    \"score\": null\\n}',\n",
       "  'template_vars': {'grade_level': 'high school', 'subject': 'biology'},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = \"The travel agency wants to ensure their translations are not only accurate but also culturally appropriate. To achieve this they are considering creating a custom metric that allows Worldwide WanderAgency to quantify how well their translations maintain cultural context and idiomatic expressions.\"\n",
    "\n",
    "# Use with default grade_level\n",
    "result = await judge.evaluate(\n",
    "    content=content,\n",
    "    metric=education_metric,\n",
    "    template_vars={\"subject\": \"biology\"}\n",
    ")\n",
    "\n",
    "result.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'DECENT',\n",
       " 'reasoning': 'The response provides a clear idea of what the travel agency is aiming to achieve, which is to ensure translations are culturally appropriate. However, it does not provide any technical details about how the custom metric would be implemented, which is crucial for evaluating the quality of the code. Therefore, it is considered decent as it sets the right direction but lacks technical depth.',\n",
       " 'score': 6.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"DECENT\",\\n    \"reasoning\": \"The response provides a clear idea of what the travel agency is aiming to achieve, which is to ensure translations are culturally appropriate. However, it does not provide any technical details about how the custom metric would be implemented, which is crucial for evaluating the quality of the code. Therefore, it is considered decent as it sets the right direction but lacks technical depth.\",\\n    \"score\": 6\\n}',\n",
       "  'template_vars': {'grade_level': 'undergraduate',\n",
       "   'subject': 'computer science'},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Override grade_level\n",
    "result = await judge.evaluate(\n",
    "    content=content,\n",
    "    metric=education_metric,\n",
    "    template_vars={\n",
    "        \"subject\": \"computer science\",\n",
    "        \"grade_level\": \"undergraduate\"  # Override default\n",
    "    }\n",
    ")\n",
    "result.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm-judge-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
